{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99af8d4c-f20f-4f60-9160-845ed67edf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0a0+git1c57644\n",
      "CUDA available: True\n",
      "GPU: \n",
      "VRAM: 274.5 GB\n",
      "DATA_DIR = dataset\n",
      "questions.json exists: True\n",
      "answers.json exists: True\n",
      "skillbank.json exists: True\n",
      "\n",
      "Cached models (7):\n",
      "  Qwen/Qwen2.5-14B-Instruct\n",
      "  Qwen/Qwen3-4B\n",
      "  Unsloth/Llama-3.1-8B-Instruct\n",
      "  google/gemma-3-12b-it\n",
      "  microsoft/Phi-4-mini-instruct\n",
      "  mistralai/Mistral-7B-Instruct-v0.3\n",
      "  unsloth/gpt-oss-20b-BF16\n"
     ]
    }
   ],
   "source": [
    "import os, torch, json\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Find data files\n",
    "for check_dir in [\".\", \"dataset\",]:\n",
    "    if os.path.exists(os.path.join(check_dir, \"questions.json\")):\n",
    "        DATA_DIR = check_dir\n",
    "        break\n",
    "else:\n",
    "    DATA_DIR = \".\"\n",
    "    print(\"WARNING: questions.json not found, set DATA_DIR manually\")\n",
    "\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"questions.json exists: {os.path.exists(os.path.join(DATA_DIR, 'questions.json'))}\")\n",
    "print(f\"answers.json exists: {os.path.exists(os.path.join(DATA_DIR, 'answers.json'))}\")\n",
    "print(f\"skillbank.json exists: {os.path.exists(os.path.join(DATA_DIR, 'skillbank.json'))}\")\n",
    "\n",
    "# Check available models in cache\n",
    "cache_dir = \"/root/.cache/huggingface/\"\n",
    "if os.path.exists(cache_dir):\n",
    "    models = [d for d in os.listdir(cache_dir) if d.startswith(\"models--\")]\n",
    "    print(f\"\\nCached models ({len(models)}):\")\n",
    "    for m in sorted(models):\n",
    "        print(f\"  {m.replace('models--', '').replace('--', '/')}\")\n",
    "else:\n",
    "    print(f\"Cache dir not found: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad750b7c-b063-414a-bb69-ca405ce21056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                             2.9.0a0+git1c57644\n",
      "torchvision                       0.23.0a0+824e8c8\n",
      "unsloth                           2025.10.9\n",
      "unsloth_zoo                       2025.10.10\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"unsloth|torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2d3fa3-84d9-44dd-8d30-b249c0773ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in hf_models for Microsoft Phi models...\n",
      "\n",
      "✅ FOUND!\n",
      "Folder: /workspace/AAIPL/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083\n",
      "Key files: ['config.json', 'model.safetensors.index.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Searching in hf_models for Microsoft Phi models...\")\n",
    "\n",
    "for root, dirs, files in os.walk('/workspace/AAIPL/hf_models'):\n",
    "    if 'config.json' in files and any(x in root.lower() for x in ['phi', 'microsoft']):\n",
    "        print(\"\\n✅ FOUND!\")\n",
    "        print(\"Folder:\", root)\n",
    "        print(\"Key files:\", [f for f in files if f in ['config.json', 'tokenizer_config.json', 'model.safetensors.index.json']])\n",
    "        break\n",
    "else:\n",
    "    print(\"No Phi model found — try this instead:\")\n",
    "    os.system(\"find /workspace -name config.json -path '*/phi*' 2>/dev/null | head -5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45052174-be70-4656-9e8e-8e1ce06836dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-15 04:51:39] INFO modeling.py:987: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea1d164276f4544adfc3dfedbd48420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"/workspace/AAIPL/hf_models/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06e9bed9-041a-4b34-bb6d-2732482ba500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Phi3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3eee4865a3341d1b70711715148cfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AAIPL/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083 does not have a padding token! Will use pad_token = <|endoftext|>.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"/workspace/AAIPL/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = False, # Enable vllm fast inference\n",
    "    max_lora_rank = 16,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "391aeeb3-d098-4cd6-b312-bb16318a7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Loading data\n",
      "============================================================\n",
      "Loaded skillbank with 53 skills\n",
      "Loaded 500 questions, 500 answers\n",
      "\n",
      "============================================================\n",
      "STEP 3: Building training data\n",
      "============================================================\n",
      "Answer examples: 500\n",
      "Question examples: 500\n",
      "Combined (shuffled): 1000\n",
      "\n",
      "============================================================\n",
      "STEP 4: Loading /workspace/AAIPL/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083\n",
      "============================================================\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "trainable params: 8,912,896 || all params: 3,844,934,656 || trainable%: 0.2318\n",
      "\n",
      "============================================================\n",
      "STEP 5: Formatting dataset\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6024f8d2f74640815ce792b46fbe6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=160):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3715a4585e2c465c9a5bc275db8c318f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dd0de215d44c7591546f77bafb5810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted: 1000 examples\n",
      "\n",
      "SAMPLE (first 800 chars):\n",
      "<|system|>You are an expert examiner designing extremely difficult MCQs for Quantitative Aptitude and Analytical Reasoning.\n",
      "## Reasoning Skills\n",
      "### General\n",
      "- Eliminate Before Selecting: Never jump to an answer. Test ALL four options — eliminate the ones that violate constraints, then confirm the survivor.\n",
      "- Assume and Contradict: If direct deduction stalls, assume each option is correct one by one and check if it leads to a contradiction with given constraints.\n",
      "- Verify Final Answer: After selecting an answer, re-check it against EVERY given constraint or statement. If any constraint is violated, the answer is wrong.\n",
      "- Read Precisely: Pay extreme attention to words like 'only', 'all', 'some', 'not', 'except', 'immediately', 'possible', 'definitely'. One word changes the entire logic.\n",
      "- AVO\n",
      "...\n",
      "\n",
      "Detected instruction_part: '<|user|>'\n",
      "Detected response_part: '<|assistant|>'\n",
      "\n",
      "============================================================\n",
      "STEP 6: Training\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93194088334a4f4086a2714ea37e5749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=64):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab25c2ee2124f1b831b3046e5154aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_on_responses_only applied\n",
      "Config: batch=16 x grad_accum=2 = effective 32\n",
      "Epochs: 3, Examples: 1000, LR: 0.0002\n",
      "\n",
      "Training started...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 3 | Total steps = 96\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 8,912,896 of 3,844,934,656 (0.23% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 04:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SFT DONE! Final loss: 0.5350\n",
      "\n",
      "============================================================\n",
      "STEP 7: Saving model\n",
      "============================================================\n",
      "LoRA saved: hf_models/phi4-mini-skills-lora\n",
      "Detected local model directory: /workspace/AAIPL/hf_models/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083\n",
      "Warning: Found cache directory /root/.cache/huggingface, but lack R/W/X permissions. Cannot use cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00002-of-00002.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied model-00001-of-00002.safetensors from local model directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:08<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/AAIPL/hf_models/phi4-mini-skills-merged`\n",
      "Merged saved: hf_models/phi4-mini-skills-merged\n",
      "\n",
      "============================================================\n",
      "ALL DONE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/root/.cache/huggingface\"\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "###############################################################\n",
    "# CONFIG — CHANGE THESE IF NEEDED\n",
    "###############################################################\n",
    "DATA_DIR = \"dataset\" \n",
    "\n",
    "SAVE_LORA = \"hf_models/phi4-mini-skills-lora\"\n",
    "SAVE_MERGED = \"hf_models/phi4-mini-skills-merged\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "BATCH_SIZE = 16 \n",
    "GRAD_ACCUM = 2 \n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "###############################################################\n",
    "\n",
    "# ── STEP 1: Load data ────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading data\")\n",
    "print(\"=\" * 60)\n",
    "with open(os.path.join(DATA_DIR, \"questions.json\")) as f:\n",
    "    questions = json.load(f)\n",
    "with open(os.path.join(DATA_DIR, \"answers.json\")) as f:\n",
    "    answers = json.load(f)\n",
    "skillbank_path = os.path.join(DATA_DIR, \"skillbank.json\")\n",
    "if os.path.exists(skillbank_path):\n",
    "    with open(skillbank_path) as f:\n",
    "        skillbank = json.load(f)\n",
    "    print(f\"Loaded skillbank with {sum(len(v) for cat in skillbank.values() for v in (cat.values() if isinstance(cat, dict) else []))} skills\")\n",
    "else:\n",
    "    skillbank = None\n",
    "    print(\"No skillbank.json found — training without skills\")\n",
    "assert len(questions) == len(answers), f\"Mismatch: {len(questions)} questions vs {len(answers)} answers\"\n",
    "print(f\"Loaded {len(questions)} questions, {len(answers)} answers\")\n",
    "\n",
    "# ── STEP 2: Build skill text ─────────────────────────────────\n",
    "TOPIC_MAP = {\n",
    "    \"Syllogisms\": \"syllogisms\",\n",
    "    \"Mixed Series (Alphanumeric)\": \"alphanumeric_series\",\n",
    "    \"Seating Arrangements (Linear, Circular)\": \"seating_arrangements\",\n",
    "    \"Blood Relations and Family Tree\": \"blood_relations\",\n",
    "}\n",
    "def get_skills(topic, agent_type=\"answer\"):\n",
    "    if skillbank is None:\n",
    "        return \"\"\n",
    "    key = TOPIC_MAP.get(topic, \"\")\n",
    "    lines = [\"\\n## Reasoning Skills\", \"### General\"]\n",
    "    for s in skillbank.get(\"general\", {}).get(\"solving\", []):\n",
    "        lines.append(f\"- {s['title']}: {s['principle']}\")\n",
    "    for s in skillbank.get(\"general\", {}).get(\"errors\", []):\n",
    "        lines.append(f\"- AVOID: {s['principle']}\")\n",
    "    if agent_type == \"question\":\n",
    "        for s in skillbank.get(\"general\", {}).get(\"question_generation\", []):\n",
    "            lines.append(f\"- {s['title']}: {s['principle']}\")\n",
    "    if key and key in skillbank:\n",
    "        lines.append(f\"### {topic}\")\n",
    "        for s in skillbank[key].get(\"solving\", []):\n",
    "            lines.append(f\"- {s['title']}: {s['principle']}\")\n",
    "        for s in skillbank[key].get(\"errors\", []):\n",
    "            lines.append(f\"- AVOID: {s['principle']}\")\n",
    "        if agent_type == \"question\":\n",
    "            for s in skillbank[key].get(\"question_generation\", []):\n",
    "                lines.append(f\"- {s['title']}: {s['principle']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ── STEP 3: Build training conversations ─────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Building training data\")\n",
    "print(\"=\" * 60)\n",
    "answer_data = []\n",
    "for q, a in zip(questions, answers):\n",
    "    topic = q[\"topic\"]\n",
    "    skills = get_skills(topic, \"answer\")\n",
    "    sys_p = (\"You are an expert in quantitative aptitude for competitive exams, \"\n",
    "             \"solving MCQs with step-by-step reasoning before selecting the correct answer.\" + skills)\n",
    "    choices_str = \" \".join(q[\"choices\"])\n",
    "    user_p = (f\"Question: {q['question']}\\nChoices: {choices_str}\\n\\n\"\n",
    "              \"Respond with JSON: {\\\"answer\\\": \\\"letter\\\", \\\"reasoning\\\": \\\"brief explanation\\\"}\")\n",
    "    letter = a[\"answer\"].strip().upper()[:1]\n",
    "    reasoning = q.get(\"explanation\", \"\") or \"Systematic analysis and elimination.\"\n",
    "    asst = json.dumps({\"answer\": letter, \"reasoning\": reasoning}, ensure_ascii=False)\n",
    "    answer_data.append({\"conversations\": [\n",
    "        {\"role\": \"system\", \"content\": sys_p},\n",
    "        {\"role\": \"user\", \"content\": user_p},\n",
    "        {\"role\": \"assistant\", \"content\": asst},\n",
    "    ]})\n",
    "\n",
    "question_data = []\n",
    "for q in questions:\n",
    "    topic = q[\"topic\"]\n",
    "    skills = get_skills(topic, \"question\")\n",
    "    sys_p = (\"You are an expert examiner designing extremely difficult MCQs \"\n",
    "             \"for Quantitative Aptitude and Analytical Reasoning.\" + skills)\n",
    "    user_p = (f\"Generate an EXTREMELY DIFFICULT MCQ on topic: {topic}.\\n\"\n",
    "              \"Respond with JSON: {\\\"topic\\\": \\\"...\\\", \\\"question\\\": \\\"...\\\", \"\n",
    "              \"\\\"choices\\\": [\\\"A) ...\\\", \\\"B) ...\\\", \\\"C) ...\\\", \\\"D) ...\\\"], \"\n",
    "              \"\\\"answer\\\": \\\"letter\\\", \\\"explanation\\\": \\\"...\\\"}\")\n",
    "    letter = q.get(\"expected_answer\", q.get(\"answer\", \"A\")).strip().upper()[:1]\n",
    "    asst = json.dumps({\"topic\": topic, \"question\": q[\"question\"], \"choices\": q[\"choices\"],\n",
    "                       \"answer\": letter, \"explanation\": q.get(\"explanation\", \"\")}, ensure_ascii=False)\n",
    "    question_data.append({\"conversations\": [\n",
    "        {\"role\": \"system\", \"content\": sys_p},\n",
    "        {\"role\": \"user\", \"content\": user_p},\n",
    "        {\"role\": \"assistant\", \"content\": asst},\n",
    "    ]})\n",
    "combined = answer_data + question_data\n",
    "random.shuffle(combined)\n",
    "print(f\"Answer examples: {len(answer_data)}\")\n",
    "print(f\"Question examples: {len(question_data)}\")\n",
    "print(f\"Combined (shuffled): {len(combined)}\")\n",
    "\n",
    "# ── STEP 4: Load model + LoRA ────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"STEP 4: Loading {MODEL_NAME}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick check that the local folder exists\n",
    "if not os.path.exists(MODEL_NAME):\n",
    "    print(f\"❌ Folder not found: {MODEL_NAME}\")\n",
    "    print(\"Run this in a new cell to see your folders:\")\n",
    "    print(\"!ls -la /workspace/AAIPL/hf_models/microsoft/\")\n",
    "    raise FileNotFoundError(f\"Model folder {MODEL_NAME} does not exist\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ── STEP 5: Format dataset ───────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Formatting dataset\")\n",
    "print(\"=\" * 60)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "dataset = Dataset.from_list(combined)\n",
    "def formatting_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Template error: {e}\")\n",
    "            texts.append(\"\")\n",
    "    return {\"text\": texts}\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_func, batched=True, remove_columns=dataset.column_names)\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "print(f\"Formatted: {len(dataset)} examples\")\n",
    "sample = dataset[\"text\"][0]\n",
    "print(f\"\\nSAMPLE (first 800 chars):\")\n",
    "print(sample[:800])\n",
    "print(\"...\\n\")\n",
    "\n",
    "INSTRUCTION_PART = None\n",
    "RESPONSE_PART = None\n",
    "marker_checks = [\n",
    "    (\"<|im_start|>user<|im_sep|>\", \"<|im_start|>assistant<|im_sep|>\"),\n",
    "    (\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n",
    "    (\"<|user|>\\n\", \"<|assistant|>\\n\"),\n",
    "    (\"<|user|>\", \"<|assistant|>\"),\n",
    "    (\"<|start_header_id|>user<|end_header_id|>\\n\\n\", \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"),\n",
    "]\n",
    "for instr, resp in marker_checks:\n",
    "    if instr in sample and resp in sample:\n",
    "        INSTRUCTION_PART = instr\n",
    "        RESPONSE_PART = resp\n",
    "        break\n",
    "print(f\"Detected instruction_part: {repr(INSTRUCTION_PART)}\")\n",
    "print(f\"Detected response_part: {repr(RESPONSE_PART)}\")\n",
    "if INSTRUCTION_PART is None:\n",
    "    print(\"WARNING: Could not detect markers! Check the sample above and set them manually.\")\n",
    "\n",
    "# ── STEP 6: Train ────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Training\")\n",
    "print(\"=\" * 60)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"sft_checkpoints\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "    ),\n",
    ")\n",
    "if INSTRUCTION_PART and RESPONSE_PART:\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part=INSTRUCTION_PART,\n",
    "        response_part=RESPONSE_PART,\n",
    "    )\n",
    "    print(\"train_on_responses_only applied\")\n",
    "FastLanguageModel.for_training(model)\n",
    "print(f\"Config: batch={BATCH_SIZE} x grad_accum={GRAD_ACCUM} = effective {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}, Examples: {len(dataset)}, LR: {LEARNING_RATE}\")\n",
    "print(\"\\nTraining started...\\n\")\n",
    "stats = trainer.train()\n",
    "print(f\"\\nSFT DONE! Final loss: {stats.training_loss:.4f}\")\n",
    "\n",
    "# ── STEP 7: Save ─────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Saving model\")\n",
    "print(\"=\" * 60)\n",
    "os.makedirs(os.path.dirname(SAVE_LORA), exist_ok=True)\n",
    "model.save_pretrained(SAVE_LORA)\n",
    "tokenizer.save_pretrained(SAVE_LORA)\n",
    "print(f\"LoRA saved: {SAVE_LORA}\")\n",
    "model.save_pretrained_merged(SAVE_MERGED, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"Merged saved: {SAVE_MERGED}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL DONE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebea2b89-085a-4a31-9705-fb8a823b75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== A-Agent ===\n",
      "Topic: Syllogisms\n",
      "Expected: C\n",
      "Model: {\"answer\": \"C\", \"reasoning\": \"Set contradiction reasoning\"}\n",
      "\n",
      "=== Q-Agent ===\n",
      "Generated: {\"question\": \"\\nSet:\\nAll A\\nSome B\\nNo C\\nAll C\\nSet contradiction:\\nSome A\\nSet conclusion:\\nNone\\nSet contradiction:\\nAll A\\nSet contradiction:\\nSome B\\nSet contradiction:\\nNone\\nSet contradiction:\\nAll A\\nSet contradiction:\\nSome C\\nSet contradiction:\\nSet contradiction:\\nNoneSet contradiction:\\nSet contradiction:\\nAll CSet contradiction:\\nNoneSet contradiction:\\nSet contradiction:\\nSome ASet contradiction:\\nSet contradiction:\\nNoneSet contradiction:\\nSet contradiction:\\nAll BSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\nSet contradiction:\\n\n",
      "\n",
      "=== Batch Test (20 questions) ===\n",
      "OK    Seating Arrangements (Lin exp=C got=C\n",
      "OK    Mixed Series (Alphanumeri exp=D got=D\n",
      "OK    Mixed Series (Alphanumeri exp=C got=C\n",
      "OK    Blood Relations and Famil exp=D got=D\n",
      "OK    Syllogisms                exp=B got=B\n",
      "OK    Syllogisms                exp=A got=A\n",
      "WRONG Seating Arrangements (Lin exp=B got=D\n",
      "OK    Blood Relations and Famil exp=C got=C\n",
      "OK    Syllogisms                exp=D got=D\n",
      "OK    Mixed Series (Alphanumeri exp=D got=D\n",
      "OK    Blood Relations and Famil exp=B got=B\n",
      "OK    Blood Relations and Famil exp=D got=D\n",
      "OK    Seating Arrangements (Lin exp=B got=B\n",
      "OK    Syllogisms                exp=C got=C\n",
      "OK    Mixed Series (Alphanumeri exp=B got=B\n",
      "WRONG Seating Arrangements (Lin exp=A got=D\n",
      "OK    Blood Relations and Famil exp=B got=B\n",
      "WRONG Mixed Series (Alphanumeri exp=A got=D\n",
      "OK    Syllogisms                exp=B got=B\n",
      "WRONG Seating Arrangements (Lin exp=B got=A\n",
      "\n",
      "Accuracy: 16/20 = 80.0%\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# CELL 3: Test the fine-tuned model\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "\n",
    "import json, re, random\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def test_one(msgs, max_tokens=256):\n",
    "    inputs = tokenizer.apply_chat_template(msgs, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(input_ids=inputs, max_new_tokens=max_tokens, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(out[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test A-agent\n",
    "q = questions[0]\n",
    "resp = test_one([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in quantitative aptitude.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Question: {q['question']}\\nChoices: {' '.join(q['choices'])}\\nJSON: {{\\\"answer\\\": \\\"letter\\\", \\\"reasoning\\\": \\\"...\\\"}}\"},\n",
    "])\n",
    "print(f\"=== A-Agent ===\")\n",
    "print(f\"Topic: {q['topic']}\")\n",
    "print(f\"Expected: {q.get('expected_answer', q.get('answer'))}\")\n",
    "print(f\"Model: {resp}\\n\")\n",
    "\n",
    "# Test Q-agent\n",
    "resp = test_one([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert examiner.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Generate an EXTREMELY DIFFICULT MCQ on topic: Syllogisms. Respond with JSON.\"},\n",
    "], max_tokens=512)\n",
    "print(f\"=== Q-Agent ===\")\n",
    "print(f\"Generated: {resp}\\n\")\n",
    "\n",
    "# Batch accuracy\n",
    "print(\"=== Batch Test (20 questions) ===\")\n",
    "indices = random.sample(range(len(questions)), min(20, len(questions)))\n",
    "correct = 0\n",
    "for idx in indices:\n",
    "    q = questions[idx]\n",
    "    expected = answers[idx][\"answer\"].strip().upper()[:1]\n",
    "    resp = test_one([\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in quantitative aptitude.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {q['question']}\\nChoices: {' '.join(q['choices'])}\\nJSON: {{\\\"answer\\\": \\\"letter\\\", \\\"reasoning\\\": \\\"...\\\"}}\"},\n",
    "    ])\n",
    "    extracted = None\n",
    "    try:\n",
    "        extracted = json.loads(resp.strip()).get(\"answer\", \"\").strip().upper()[:1]\n",
    "    except Exception:\n",
    "        m = re.search(r'\"answer\"\\s*:\\s*\"([A-Da-d])\"', resp)\n",
    "        if m: extracted = m.group(1).upper()\n",
    "    ok = extracted == expected\n",
    "    if ok: correct += 1\n",
    "    print(f\"{'OK' if ok else 'WRONG':5s} {q['topic'][:25]:25s} exp={expected} got={extracted}\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{len(indices)} = {correct/len(indices)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06fce8-92af-4c86-9031-d47a239ddbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"/workspace/AAIPL/hf_models/phi4-mini-skills-merged\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = False, # Enable vllm fast inference\n",
    "    max_lora_rank = 16,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c55865-1480-499b-a608-6256cc638213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
